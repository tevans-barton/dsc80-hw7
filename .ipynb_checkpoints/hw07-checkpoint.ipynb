{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSC 80: Homework 07\n",
    "\n",
    "### Due Date: Monday, Feb 25 12:00PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "Much like in DSC 10, this Jupyter Notebook contains the statements of the homework problems and provides code and markdown cells to display your answers to the problems. Unlike DSC 10, the notebook is *only* for displaying a readable version of your final answers. The coding work will be developed in an accompanying `hw0X.py` file, that will be imported into the current notebook. (`X` is a homework number)\n",
    "\n",
    "Homeworks and programming assignments will be graded in (at most) two ways:\n",
    "1. The functions and classes in the accompanying python file will be tested (a la DSC 20),\n",
    "2. The notebook will be graded (for graphs and free response questions).\n",
    "\n",
    "\n",
    "**Do not change the function names in the `*.py` file**\n",
    "- The functions in the `*.py` file are how your assignment is graded, and they are graded by their name. The dictionary at the end of the file (`GRADED FUNCTIONS`) contains the \"grading list\". The final function in the file allows your doctests to check that all the necessary functions exist.\n",
    "- If you changed something you weren't supposed to, just use git to revert!\n",
    "\n",
    "**Tips for working in the Notebook**:\n",
    "- The notebooks serve to present you the questions and give you a place to present your results for later review.\n",
    "- The notebook on *HW assignments* are not graded (only the `.py` file).\n",
    "- Notebooks for PAs will serve as a final report for the assignment, and contain conclusions and answers to open ended questions that are graded.\n",
    "- The notebook serves as a nice environment for 'pre-development' and experimentation before designing your function in your `.py` file.\n",
    "\n",
    "**Tips for developing in the .py file**:\n",
    "- Do not change the function names in the starter code; grading is done using these function names.\n",
    "- Do not change the docstrings in the functions. These are there to tell you if your work is on the right track!\n",
    "- You are encouraged to write your own additional functions to solve the HW! \n",
    "    - Developing in python usually consists of larger files, with many short functions.\n",
    "    - You may write your other functions in an additional `.py` file that you import in `hw0X.py` (much like we do in the notebook).\n",
    "- Always document your code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hw07 as hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API requests\n",
    "\n",
    "**Question 1**\n",
    "\n",
    "You realized that you have a lot of free time and you decided to learn a new language. Since you are not sure which language to choose, you decided to check out the News API: `https://newsapi.org/`. The NewsAPI allows you to make requests to a URL and get a response based on the parameters and values you request. In order to send requests you must click on the `Get API Key` button and fill out the form to access a key. Save this key; you need it to access the API. You will have it saved in your account as well. \n",
    "\n",
    "I'd suggest you explore the site (especially `Get started page`). Once you have an idea what is required to send requests, you are ready to test things out. Since you need to decide what language to learn, you decided to compare the number of articles in different languages: \n",
    "\n",
    "1. Write a function `send_requests` that takes an `apiKey` as a parameter, and a variable number of arguments that represent languages for your requests. (use \\*args to represent a variable number of arguments. <a href= \": https://pythontips.com/2013/08/04/args-and-kwargs-in-python-explained/\">Link for help</a>). This function should return a list of dictionaries, where each key-value pair is a languages parameter and the corresponding Response object.  \n",
    "\n",
    "For example, after calling `output = send_requests('625f44103f1c4fdca96844baefad03f5',\"ru\", \"fr\") ` the *output* is ```[{'ru': <Response [200]>}, {'fr': <Response [200]>}]```\n",
    "\n",
    "\n",
    "2. Write a function `gather_info` that takes a list like `outputs` and returns a list with:\n",
    "* language that has the most number of articles (based on \"Total Results\")\n",
    "* Each article was taken from a certain source (url). You need to find the most popular base url (without http) for each language.\n",
    "\n",
    "For languages `ru` and `fr` the output was `['fr', 'lenta.ru', 'www.lequipe.fr']`\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. You only can send requests in one function. One request per language. \n",
    "2. Do not compare your results with the given outputs. The news is updated often.\n",
    "3. You must have at least two helper methods. \n",
    "4. Keep your apikey hidden. This means that you can not use it explicitly in the doctests.  You need to set an environment variable to the value of your unique apikey, which you will get from the newsapi site, and then use os.environ['name of your enviroment variable'] to obtain the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = hw.send_requests('e2210554892140e9bd1b8c87f7242100', \"ru\", \"fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fr', 'lenta.ru', 'www.lequipe.fr']"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hw.gather_info(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['www.liberation.fr',\n",
       " 'www.liberation.fr',\n",
       " 'www.lequipe.fr',\n",
       " 'www.lequipe.fr',\n",
       " 'www.lequipe.fr',\n",
       " 'www.lequipe.fr',\n",
       " 'www.lequipe.fr',\n",
       " 'www.liberation.fr',\n",
       " 'next.liberation.fr',\n",
       " 'www.lesechos.fr',\n",
       " 'www.lequipe.fr',\n",
       " 'www.lequipe.fr',\n",
       " 'www.liberation.fr',\n",
       " 'www.sudouest.fr',\n",
       " 'www.lesechos.fr',\n",
       " 'www.lemonde.fr',\n",
       " 'www.francetvinfo.fr',\n",
       " 'www.lemonde.fr',\n",
       " 'www.lequipe.fr',\n",
       " 'www.lesechos.fr']"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hw.get_urls(json.loads(responses[1]['fr'].text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The power of REGEX\n",
    "\n",
    "**Question 2**\n",
    "\n",
    "You start with some basic regular expression exercises to get some practice using them. You will find function stubs and related doctests in the starter code. \n",
    "\n",
    "\n",
    "Your pattern should catch (in a given text):\n",
    "\n",
    "**Question 1:** *abc* pattern.\n",
    "\n",
    "**Question 2:** three digits in a row.\n",
    "\n",
    "**Question 3:** any string that has \".\" as the fourth character.\n",
    "\n",
    "**Question 4:** any string that starts with *c*, *m*, or *f* and is followed by *an*.\n",
    "\n",
    "**Question 5:** any string that contains *og* and does not start with a *b*.\n",
    "\n",
    "**Question 6:** words that begin with a capital letter.\n",
    "\n",
    "**Question 7:** any version of the word *wazup*, as long as more than 1 z is used.\n",
    "\n",
    "**Question 8:** any string containing, in order, at least one *a*, and at least one *b* or *c*.\n",
    "\n",
    "**Question 9:** any variation of the string *N file(s) found?*.\n",
    "\n",
    "**Question 10:** any enumeration follow by a whitespace character followed by any text.\n",
    "\n",
    "**Question 11:** Your pattern should match strings that begin with the word *Mission:* and end with the word *successful*. Words in the middle are allowed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex groups: extracting personal information from messy data\n",
    "\n",
    "**Question 3**\n",
    "\n",
    "The file in `data/messy.txt` contains personal information from a fictional website that a user scraped from webserver logs. Within this dataset, there are four fields that interest you:\n",
    "1. Email Addresses (assume they are alphanumeric user-names and domain-names),\n",
    "2. [Social Security Numbers](https://en.wikipedia.org/wiki/Social_Security_number#Structure)\n",
    "3. Bitcoin Addresses (alpha-numeric strings of long length)\n",
    "4. Street Addresses\n",
    "\n",
    "Create a function `extract_personal` that takes in a string like `open('data/messy.txt').read()` and returns a tuple of four separate lists containing values of the 4 pieces of information listed above (in the order given). Do **not** keep empty values.\n",
    "\n",
    "*Hint*: There are multiple \"delimiters\" in use in the file; there are few enough of them that you can safely determine what they are.\n",
    "\n",
    "*Note:* Since this data is messy/corrupted, your function will be allowed to miss ~5% of the records in each list. Good spot checking using certain useful substrings (e.g. `@` for emails) should help assure correctness! Your function will be tested on a sample of the file `messy.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = open('data/messy.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1\\t4/12/2018\\tLorem ipsum dolor sit amet, consectetuer adipiscing elit. Proin risus. Praesent lectus.\\n\\nVestibulum quam sapien| varius ut, blandit non, interdum in, ante. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia Curae; Duis faucibus accumsan odio. Curabitur convallis.|dottewell0@gnu.org\\toR1mOq,!@#$%^&*(),[{bitcoin:18A8rBU3wvbLTSxMjqrPNc9mvonpA4XMiv\\tIP:192.232.9.210\\tccn:3563354617955160|ssn:380-09-9403}]|05-6609813,814 Monterey Court\\n2\\t12/18/2018\\tSuspendisse potenti. In eleifend quam a odio. In hac habitasse platea dictumst.\\n\\nMaecenas ut massa quis augue luctus tincidunt. Nulla mollis molestie lorem. Quisque ut erat.,bassiter1@sphinn.com\\tc5KvmarHX3o,test\\u2060test\\u202b,[{bitcoin:1EB7kYpnfJSqS7kUFpinsmPF3uiH9sfRf1,IP:20.73.13.197|ccn:3542723823957010\\tssn:118-12-8276}#{bitcoin:1E5fev4boabWZmXvHGVkHcNJZ2tLnpM6Zv*IP:238.206.212.148\\tccn:337941898369615,ssn:427-22-9352}#{bitcoin:1DqG3WcmGw74PjptjzcAmxGFuQdvWL7RCC,IP:171.241.15.98\\tccn:3574672962323693,ssn:649-16-224'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bitcoin:14F$jk%^3\\\\t,test@test.com,lkj5r%ji|ssn:423-00-9575,530 High Street\\n'"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = os.path.join('data', 'messy.test.txt')\n",
    "s = open(fp, encoding='utf8').read()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content in Amazon review data\n",
    "\n",
    "**Question 4**\n",
    "\n",
    "The dataset `reviews.txt` contains [Amazon reviews](http://jmcauley.ucsd.edu/data/amazon/) for ~200k phones and phone accessories. This dataset has been \"cleaned\" for you. The goal of this section is to create a function that takes in the review dataset and a review and returns the word that \"best summarizes the review\" using TF-IDF.'\n",
    "\n",
    "1. Create a function `tfidf_data(review, reviews)` that takes a review as well as the review data and returns a dataframe:\n",
    "    - indexed by the words in `review`,\n",
    "    - with columns given by (a) the number of times each word is found in the review (`cnt`), (b) the term frequency for each word (`tf`), (c) the inverse document frequency for each word (`idf`), and (d) the TF-IDF for each word (`tfidf`).\n",
    "    \n",
    "2. Create a function `relevant_word(tfidf_data)` which takes in a dataframe as above and returns the word that \"best summarizes the review\" described by `tfidf_data`.\n",
    "\n",
    "\n",
    "*Note:* Use this function to \"cluster\" review types -- run it on a sample of reviews and see which words come up most. Unfortunately, you will likely have to change your code from your answer above to run it on the entire dataset (to do this, you should compute as many of the frequencies \"ahead of time\" and look them up when needed; you should also likely filter out words that occur \"rarely\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('data/reviews.txt', header=None, squeeze=True)\n",
    "review = open('data/review.txt').read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = review.split()\n",
    "wordSeries = pd.Series(words)\n",
    "index = wordSeries.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = hw.tfidf_data(review, reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chunk'"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.sort_values('tfidf', ascending = False).index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet Analysis: Internet Research Agency\n",
    "\n",
    "The dataset `data/ira.csv` contains tweets tagged by Twitter as likely being posted by the *Internet Research Angency* (the tweet factory facing allegations for attempting to influence US political elections).\n",
    "\n",
    "The questions in this section will focus on the following:\n",
    "1. We will look at the hashtags present in the text and trends in their makeup.\n",
    "2. We will prepare this dataset for modeling by creating features out of the text fields.\n",
    "\n",
    "**Question 5 (HashTags)**\n",
    "\n",
    "* Create a function `hashtag_list` that takes in a column of tweet-text and returns a column containing the list of hashtags present in the tweet text.\n",
    "\n",
    "* Create a function `most_common_hashtag` that takes in a column of hashtag-lists (the output above) and returns a column consisting a single hashtag from the tweet-text. \n",
    "    - If the text has no hashtags, the entry should be `NaN`,\n",
    "    - If the text has one distinct hashtag, the entry should contain that hashtag,\n",
    "    - If the text has more than one hashtag, the entry should be the most common hashtag (among all tweets). If there is a tie for most common, any of the most common can be returned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "ira = pd.read_csv('data/ira.csv', names=['id', 'name', 'date', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = hw.hashtag_list(ira['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdict = {5 : 'hello', 8 : 'goobye', 6 : 'why', 20 : 'it really', 9 : 'yuh'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                            CatTV\n",
       "1                              NaN\n",
       "2                             tech\n",
       "3                             news\n",
       "4            IHatePokemonGoBecause\n",
       "5                           health\n",
       "6                              NaN\n",
       "7             IWouldPreferToForget\n",
       "8                       NowPlaying\n",
       "9                 AthleticsTVShows\n",
       "10            HillaryRottenClinton\n",
       "11                             NaN\n",
       "12                      TrumpTapes\n",
       "13                   entertainment\n",
       "14                             NaN\n",
       "15                             NaN\n",
       "16                             NaN\n",
       "17                             NaN\n",
       "18                          sports\n",
       "19                        politics\n",
       "20                             NaN\n",
       "21                             NaN\n",
       "22                      IslamKills\n",
       "23                       SlowJamTV\n",
       "24       assaultatspringvalleyhigh\n",
       "25                   WakeUpAmerica\n",
       "26                             NaN\n",
       "27                             NaN\n",
       "28                             NaN\n",
       "29                 OscarHasNoColor\n",
       "                   ...            \n",
       "89970                         news\n",
       "89971                          NaN\n",
       "89972              HateToEatAndRun\n",
       "89973                          NaN\n",
       "89974                          NaN\n",
       "89975         ThingsIWontTellMyDad\n",
       "89976                          NaN\n",
       "89977                          NaN\n",
       "89978                          NaN\n",
       "89979           ChristmasAftermath\n",
       "89980                          NaN\n",
       "89981         RejectedDebateTopics\n",
       "89982                          NaN\n",
       "89983                       health\n",
       "89984      MyBestFriendIsntAllowed\n",
       "89985                          NaN\n",
       "89986              refugeeswelcome\n",
       "89987                       events\n",
       "89988                          NaN\n",
       "89989                         Ohio\n",
       "89990             BlackLivesMatter\n",
       "89991                        IHate\n",
       "89992           MakeMusicReligious\n",
       "89993                 2016In4Words\n",
       "89994                          NaN\n",
       "89995                     politics\n",
       "89996          ThingsYouCantIgnore\n",
       "89997                          NaN\n",
       "89998                 rantfortoday\n",
       "89999                         news\n",
       "Name: text, Length: 90000, dtype: object"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hw.most_common_hashtag(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6 (Features)**\n",
    "\n",
    "Now create a dataframe of features from the `ira` data.  That is create a function `create_features` that takes in the `ira` data and returns a dataframe with the same index as `ira` (i.e. the rows correspond to the same tweets) and the following columns:\n",
    "* `num_hashtags` gives the number of hashtags present in a tweet,\n",
    "* `mc_hashtag` gives the most common hashtag associated to a tweet (as given by the problem above),\n",
    "* `num_tags` gives the number of tags a given tweet has (look for the presence of `@`),\n",
    "* `num_links` gives the number of hyper-links present in a given tweet,\n",
    "* A boolean column `is_retweet` that describes if the given tweet is a retweet (i.e. `RT`),\n",
    "* A 'clean' text field `text` that contains the tweet text with:\n",
    "    - The non-alphanumeric characters removed (except spaces),\n",
    "    - The characters all lowercase,\n",
    "    - All the meta-information above (Retweet info, tags, hyperlinks, hashtags) removed.\n",
    "\n",
    "*Note:* You should make a helper function for each column.\n",
    "\n",
    "*Note:* This will take a while to run on the entire dataset -- test it on a small sample first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata = [['RT @DSC80: Text-cleaning is cool! #NLP https://t.co/xsfdw88d #NLP1 #NLP1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(testdata, columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = hw.create_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>num_hashtags</th>\n",
       "      <th>mc_hashtags</th>\n",
       "      <th>num_tags</th>\n",
       "      <th>num_links</th>\n",
       "      <th>is_retweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text cleaning is cool</td>\n",
       "      <td>3</td>\n",
       "      <td>NLP1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    text  num_hashtags mc_hashtags  num_tags  num_links  \\\n",
       "0  text cleaning is cool             3        NLP1         1          1   \n",
       "\n",
       "   is_retweet  \n",
       "0        True  "
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "anscols = ['text', 'num_hashtags', 'mc_hashtags', 'num_tags', 'num_links', 'is_retweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>num_hashtags</th>\n",
       "      <th>mc_hashtags</th>\n",
       "      <th>num_tags</th>\n",
       "      <th>num_links</th>\n",
       "      <th>is_retweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text cleaning is cool</td>\n",
       "      <td>3</td>\n",
       "      <td>NLP1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    text  num_hashtags mc_hashtags  num_tags  num_links  \\\n",
       "0  text cleaning is cool             3        NLP1         1          1   \n",
       "\n",
       "   is_retweet  \n",
       "0        True  "
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ansdata = [['text cleaning is cool', 3, 'NLP1', 1, 1, True]]\n",
    "ans = pd.DataFrame(ansdata, columns=anscols)\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(out == ans).all().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Congratulations, you're done with the homework***\n",
    "\n",
    "Now, run your doctests and upload hw07.py to GradeScope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
